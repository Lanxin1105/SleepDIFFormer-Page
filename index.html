<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>SleepDIFFormer</title>
  <style>
    body {
      font-family: Arial, sans-serif;
      text-align: center;
      margin: 40px;
    }
    h1 {
      font-size: 36px;
    }
    .authors {
      font-size: 18px;
      color: #555;
    }
    .buttons a {
      display: inline-block;
      margin: 10px;
      padding: 10px 20px;
      background: #333;
      color: white;
      text-decoration: none;
      border-radius: 5px;
    }
    img {
      margin-top: 20px;
      max-width: 90%;
    }
    .section {
      max-width: 800px;
      margin: 0 auto;
      text-align: left;
    }
    h2 {
      margin-top: 50px;
    }
  </style>
</head>
<body>

  <h1>SleepDIFFormer: Multimodal Differential Transformer for Sleep Staging</h1>

  <div class="authors">
    Lanxin 1105, Your Teammates  
    <br>Southeast University
  </div>

  <div class="buttons">
    <a href="pdf-paper-link" target="_blank">ðŸ“„ Paper</a>
    <a href="https://arxiv.org/abs/paper-link" target="_blank">ðŸ”— arXiv</a>
    <a href="https://github.com/Ben1001409/SleepDIFFormer" target="_blank">ðŸ’» Code</a>
  </div>

  <img src="your-image.png" alt="Project Figure">

  <div class="section">
    <h2>Abstract</h2>
    <p>
      Manual scoring requires sleep expert to inspect characteristics of different sleep stages in EEG and classify those 30 seconds epochs into sleep stages, which are time-consuming and prone to error. Nowadays, automated model that involved machine learning and deep learning methods are actively explored by researchers. However, existing automated approaches does not take into account the non-stationarity nature of bio-signals such as Electroencephalography (EEG) and Electrooculography (EOG), disregard the ability of the model to generalize on other unseen dataset and specific architecture tailored for EEG signals remains unexplored. This research proposes a novel automated multivariate transformer-based model to close these gaps.</p>    
    <p>  
      Inspired by general time-series forecasting models, this study introducing global token for each epoch of different modalities to reconcile information between different signal type. Considering the variability in bio-signals, behaviour of attention mechanism in traditional transformer, background noise and artifacts, differential attention mechanism was designed in the architecture to suppress the attention noise and increase signal-to-noise (SNR) ratio. To closely matched with real-world clinical application, the model learns domain invariant features through features alignment using statistical approach between different training datasets with different distribution to generalize well on target dataset, which are not seen by model. The model was validated on 5 different sleep staging datasets, surpassing the performance of state-of-the-art.</p>
    </p>
  </div>

</body>

<!-- Experimental Section -->
<h2>Experimental evidences</h2>
<p>
  In this section, we present the main results of the proposed SleepDIFFormer model on multiple datasets.  
  Our method demonstrates strong generalization and alignment capabilities, achieving state-of-the-art results in most evaluation settings.
</p>

<img src="figures/result.png" alt="Table 1" style="max-width:95%; margin-top:20px;">

<!-- BibTeX Section -->
<h2>BibTeX</h2>
<pre style="background:#f5f5f5; padding:15px; text-align:left; overflow-x:auto;">
@article{xxx2025sleepdifformer,
  author  = {Name},
  title   = {SleepDIFFormer: Multimodal Differential Transformer for Sleep Staging},
  journal = {arXiv preprint arXiv:xxxx.xxxxx},
  year    = {2025}
}
</pre>

</html>
